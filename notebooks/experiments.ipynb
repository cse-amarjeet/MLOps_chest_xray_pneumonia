{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de4ae208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amar/miniconda3/envs/mlprojecta/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/paultimothymooney/chest-xray-pneumonia?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.29G/2.29G [04:05<00:00, 10.0MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default path:  /home/amar/.cache/kagglehub/datasets/paultimothymooney/chest-xray-pneumonia/versions/2\n",
      "Dataset copied to: ../data/raw/\n"
     ]
    }
   ],
   "source": [
    "#Download the data\n",
    "\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Download to kagglehub's default cache location\n",
    "cached_path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
    "\n",
    "print('default path: ',cached_path)\n",
    "# Your desired custom folder\n",
    "destination = \"../data/raw/\"\n",
    "\n",
    "# Copy dataset to your preferred location\n",
    "# shutil.copytree(cached_path, destination, dirs_exist_ok=True)\n",
    "shutil.move(cached_path, destination)\n",
    "\n",
    "print(f\"Dataset moved to: {destination}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66441808",
   "metadata": {},
   "source": [
    "### Data preprocessing steps\n",
    "\n",
    "- Inspect images: Detect corrupted files\n",
    "- Resize to fixed size: Standard input for CNNs\n",
    "- Normalize:\tFaster convergence\n",
    "- Convert to Tensor:\tRequired for model input\n",
    "- Data Augmentation (train only):\tImprove generalization\n",
    "- Balanced classes:\tAvoid bias towards one class\n",
    "- Dataloaders:\tEfficient training pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69a5a197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Corrupted image found\n"
     ]
    }
   ],
   "source": [
    "# see if there any image is corrupted\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def inspect_images(parent_dir):\n",
    "    for split in os.listdir(parent_dir):  # train, test, val\n",
    "        split_path = os.path.join(parent_dir, split)\n",
    "        if not os.path.isdir(split_path):\n",
    "            continue\n",
    "        for cls in os.listdir(split_path):  # NORMAL, PNEUMONIA\n",
    "            class_path = os.path.join(split_path, cls)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "            for img_file in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                try:\n",
    "                    with Image.open(img_path) as img:\n",
    "                        img.verify()  # Check for corruption\n",
    "                except Exception as e:\n",
    "                    print(f\"Corrupted image: {img_path}, Error: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "DATA_DIR = \"/home/amar/amar/MLOps_project/MLOps_chest_xray_pneumonia/data/raw/chest_xray\"\n",
    "inspect_images(parent_dir)\n",
    "print(\"No Corrupted image found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7529559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Checking for corrupted images...\n",
      "ðŸ“¦ Loading datasets...\n",
      "ðŸšš Creating dataloaders...\n",
      "âœ… Preprocessing complete.\n",
      "Train samples: 5216, Val: 16, Test: 624\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ========== Step 1: Check for corrupted images ==========\n",
    "def check_corrupted_images(root_dir):\n",
    "    print(\"ðŸ” Checking for corrupted images...\")\n",
    "    for phase in ['train', 'val', 'test']:\n",
    "        phase_path = os.path.join(root_dir, phase)\n",
    "        for cls in os.listdir(phase_path):\n",
    "            class_path = os.path.join(phase_path, cls)\n",
    "            for img_file in os.listdir(class_path):\n",
    "                img_path = os.path.join(class_path, img_file)\n",
    "                try:\n",
    "                    img = Image.open(img_path)\n",
    "                    img.verify()\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Corrupted image found: {img_path} â€” {e}\")\n",
    "\n",
    "# ========== Step 2: Define Transforms ==========\n",
    "IMG_SIZE = 224\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# ========== Step 3: Load Datasets ==========\n",
    "def load_datasets(data_dir):\n",
    "    print(\"ðŸ“¦ Loading datasets...\")\n",
    "    train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_transform)\n",
    "    val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform=val_test_transform)\n",
    "    test_dataset = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=val_test_transform)\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# ========== Step 4: Create DataLoaders ==========\n",
    "def create_dataloaders(train_ds, val_ds, test_ds, batch_size=32):\n",
    "    print(\"ðŸšš Creating dataloaders...\")\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ========== Main Execution ==========\n",
    "if __name__ == \"__main__\":\n",
    "    check_corrupted_images(DATA_DIR)\n",
    "    \n",
    "    train_ds, val_ds, test_ds = load_datasets(DATA_DIR)\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(train_ds, val_ds, test_ds)\n",
    "\n",
    "    print(\"âœ… Preprocessing complete.\")\n",
    "    print(f\"Train samples: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2847efcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amar/miniconda3/envs/mlprojecta/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/amar/miniconda3/envs/mlprojecta/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /home/amar/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.8M/30.8M [00:03<00:00, 10.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163/163 [11:36<00:00,  4.28s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1108, Accuracy: 96.09%\n",
      "Val   Loss: 0.8385, Accuracy: 62.50%\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163/163 [14:02<00:00,  5.17s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0519, Accuracy: 97.99%\n",
      "Val   Loss: 0.0787, Accuracy: 100.00%\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163/163 [14:01<00:00,  5.16s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0430, Accuracy: 98.47%\n",
      "Val   Loss: 0.3391, Accuracy: 75.00%\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163/163 [13:52<00:00,  5.11s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0300, Accuracy: 99.06%\n",
      "Val   Loss: 0.5057, Accuracy: 75.00%\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163/163 [13:25<00:00,  4.94s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0302, Accuracy: 98.87%\n",
      "Val   Loss: 0.5690, Accuracy: 62.50%\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163/163 [13:38<00:00,  5.02s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0210, Accuracy: 99.23%\n",
      "Val   Loss: 0.0197, Accuracy: 100.00%\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163/163 [16:01<00:00,  5.90s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0155, Accuracy: 99.50%\n",
      "Val   Loss: 0.1347, Accuracy: 87.50%\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163/163 [15:11<00:00,  5.59s/it]\n",
      "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0234, Accuracy: 99.25%\n",
      "Val   Loss: 0.1273, Accuracy: 93.75%\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 68/163 [05:50<08:14,  5.20s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===========================\n",
    "# Load Pretrained DenseNet121\n",
    "# ===========================\n",
    "def get_model():\n",
    "    model = models.densenet121(pretrained=True)\n",
    "    num_features = model.classifier.in_features\n",
    "    model.classifier = nn.Linear(num_features, 2)  # 2 classes: Normal, Pneumonia\n",
    "    return model\n",
    "\n",
    "# ===========================\n",
    "# Training Function\n",
    "# ===========================\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / len(dataloader.dataset)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# ===========================\n",
    "# Validation Function\n",
    "# ===========================\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / len(dataloader.dataset)\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# ===========================\n",
    "# Main Training Loop\n",
    "# ===========================\n",
    "def train_model(model, train_loader, val_loader, device, epochs=10, lr=1e-4):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# ===========================\n",
    "# Example Entry Point\n",
    "# ===========================\n",
    "if __name__ == \"__main__\":\n",
    "    # from pneumonia_preprocessing import train_loader, val_loader  # assumes same file structure\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = get_model()\n",
    "\n",
    "    trained_model = train_model(model, train_loader, val_loader, device, epochs=10)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(trained_model.state_dict(), \"densenet_pneumonia.pth\")\n",
    "    print(\"âœ… Model saved as densenet_pneumonia.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f29bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160567b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
